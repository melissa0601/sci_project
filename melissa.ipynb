{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8eb526",
   "metadata": {},
   "source": [
    "# Complete DAX 40 Comprehensive Portfolio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2428da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install yfinance pandas numpy matplotlib pyplot scipy.optimize seaborn requests StringIO zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7157e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import requests\n",
    "from io import StringIO\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# DAX 40 companies information\n",
    "dax40_info = {\n",
    "    'Adidas': {'sector': 'Apparel', 'ticker': 'ADS.DE'},\n",
    "    'Airbus': {'sector': 'Aerospace & Defence', 'ticker': 'AIR.DE'},\n",
    "    'Allianz': {'sector': 'Financial Services', 'ticker': 'ALV.DE'},\n",
    "    'BASF': {'sector': 'Chemicals', 'ticker': 'BAS.DE'},\n",
    "    'Bayer': {'sector': 'Pharmaceuticals', 'ticker': 'BAYN.DE'},\n",
    "    'Beiersdorf': {'sector': 'Consumer goods', 'ticker': 'BEI.DE'},\n",
    "    'BMW': {'sector': 'Automotive', 'ticker': 'BMW.DE'},\n",
    "    'Brenntag': {'sector': 'Distribution', 'ticker': 'BNR.DE'},\n",
    "    'Commerzbank': {'sector': 'Financial Services', 'ticker': 'CBK.DE'},\n",
    "    'Continental': {'sector': 'Automotive', 'ticker': 'CON.DE'},\n",
    "    'Covestro': {'sector': 'Chemicals', 'ticker': '1COV.DE'},\n",
    "    'Daimler Truck': {'sector': 'Automotive', 'ticker': 'DTG.DE'},\n",
    "    'Deutsche Bank': {'sector': 'Financial Services', 'ticker': 'DBK.DE'},\n",
    "    'Deutsche BÃ¶rse': {'sector': 'Financial Services', 'ticker': 'DB1.DE'},\n",
    "    'Deutsche Post': {'sector': 'Logistics', 'ticker': 'DHL.DE'},\n",
    "    'Deutsche Telekom': {'sector': 'Telecommunication', 'ticker': 'DTE.DE'},\n",
    "    'E.ON': {'sector': 'Utilities', 'ticker': 'EOAN.DE'},\n",
    "    'Fresenius': {'sector': 'Healthcare', 'ticker': 'FRE.DE'},\n",
    "    'Hannover Re': {'sector': 'Insurance', 'ticker': 'HNR1.DE'},\n",
    "    'Heidelberg Materials': {'sector': 'Construction Materials', 'ticker': 'HEI.DE'},\n",
    "    'Henkel': {'sector': 'Consumer Goods', 'ticker': 'HEN3.DE'},\n",
    "    'Infineon Technologies': {'sector': 'Technology', 'ticker': 'IFX.DE'},\n",
    "    'Mercedes-Benz Group': {'sector': 'Automotive', 'ticker': 'MBG.DE'},\n",
    "    'Merck': {'sector': 'Pharmaceuticals', 'ticker': 'MRK.DE'},\n",
    "    'MTU Aero Engines': {'sector': 'Aerospace & Defence', 'ticker': 'MTX.DE'},\n",
    "    'Munich Re': {'sector': 'Financial Services', 'ticker': 'MUV2.DE'},\n",
    "    'Porsche': {'sector': 'Automotive', 'ticker': 'P911.DE'},\n",
    "    'Porsche SE': {'sector': 'Automotive', 'ticker': 'PAH3.DE'},\n",
    "    'Qiagen': {'sector': 'Biotech', 'ticker': 'QIA.DE'},\n",
    "    'Rheinmetall': {'sector': 'Aerospace & Defence', 'ticker': 'RHM.DE'},\n",
    "    'RWE': {'sector': 'Utilities', 'ticker': 'RWE.DE'},\n",
    "    'SAP': {'sector': 'Technology', 'ticker': 'SAP.DE'},\n",
    "    'Sartorius': {'sector': 'Medical Technology', 'ticker': 'SRT3.DE'},\n",
    "    'Siemens': {'sector': 'Industrials', 'ticker': 'SIE.DE'},\n",
    "    'Siemens Energy': {'sector': 'Energy technology', 'ticker': 'ENR.DE'},\n",
    "    'Siemens Healthineers': {'sector': 'Medical Equipment', 'ticker': 'SHL.DE'},\n",
    "    'Symrise': {'sector': 'Chemicals', 'ticker': 'SY1.DE'},\n",
    "    'Volkswagen Group': {'sector': 'Automotive', 'ticker': 'VOW3.DE'},\n",
    "    'Vonovia': {'sector': 'Real Estate', 'ticker': 'VNA.DE'},\n",
    "    'Zalando': {'sector': 'E-Commerce', 'ticker': 'ZAL.DE'}\n",
    "}\n",
    "\n",
    "# Extract tickers and create a mapping of tickers to sectors\n",
    "dax40_tickers = [company['ticker'] for company in dax40_info.values()]\n",
    "ticker_to_sector = {company['ticker']: company['sector'] for company in dax40_info.values()}\n",
    "\n",
    "def load_dax40_data(tickers, start_date, end_date):\n",
    "    data = yf.download(tickers, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-12-31'\n",
    "dax40_data = load_dax40_data(dax40_tickers, start_date, end_date)\n",
    "\n",
    "# Calculate returns\n",
    "returns = dax40_data['Adj Close'].pct_change().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ff077",
   "metadata": {},
   "source": [
    "## 1. Sector-Based Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6871e11",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Group returns by sector\n",
    "sector_returns = returns.groupby(ticker_to_sector, axis=1).mean()\n",
    "\n",
    "print(\"Sector-based returns calculated.\")\n",
    "print(f\"Shape of sector returns data: {sector_returns.shape}\")\n",
    "print(\"\\nUnique sectors:\")\n",
    "print(sector_returns.columns.tolist())\n",
    "\n",
    "# Visualize sector performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "(1 + sector_returns).cumprod().plot()\n",
    "plt.title('Cumulative Returns by Sector')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cecc509",
   "metadata": {},
   "source": [
    "## 2. Fama-French FIve-Factor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1a56e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO, StringIO\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "def get_french_data(url):\n",
    "    response = requests.get(url)\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zip_file:\n",
    "        file_name = zip_file.namelist()[0]\n",
    "        with zip_file.open(file_name) as file:\n",
    "            content = file.read().decode('utf-8')\n",
    "\n",
    "    lines = content.split('\\n')\n",
    "    data_lines = []\n",
    "    start_processing = False\n",
    "    for line in lines:\n",
    "        if not start_processing:\n",
    "            if any(col in line for col in ['Date', 'Mkt-RF', 'SMB', 'HML', 'RF', 'Mom']):\n",
    "                start_processing = True\n",
    "                data_lines.append(line)\n",
    "        elif start_processing:\n",
    "            if line.strip() == '':\n",
    "                break\n",
    "            if not line.startswith('Copyright'):\n",
    "                data_lines.append(line)\n",
    "\n",
    "    df = pd.read_csv(StringIO('\\n'.join(data_lines)), index_col=0, na_values=['-99.99', '-999'])\n",
    "    \n",
    "    df.index = pd.to_datetime(df.index, format='%Y%m%d')\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df = df.div(100)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_factor_exposures(returns, factors):\n",
    "    exposures = {}\n",
    "    for stock in returns.columns:\n",
    "        model = sm.OLS(returns[stock], sm.add_constant(factors)).fit()\n",
    "        exposures[stock] = model.params\n",
    "    exposures_df = pd.DataFrame(exposures).T\n",
    "    return exposures_df\n",
    "\n",
    "# URLs for Fama-French factors\n",
    "ff_url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_daily_CSV.zip\"\n",
    "momentum_url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Momentum_Factor_daily_CSV.zip\"\n",
    "\n",
    "# Get Fama-French and momentum data\n",
    "ff_factors = get_french_data(ff_url)\n",
    "momentum_factor = get_french_data(momentum_url)\n",
    "\n",
    "# Merge factors data\n",
    "factors = ff_factors.join(momentum_factor['Mom'], how='inner')\n",
    "\n",
    "# Filter factors data to match the date range of stock returns\n",
    "start_date = returns.index.min()\n",
    "end_date = returns.index.max()\n",
    "factors = factors[(factors.index >= start_date) & (factors.index <= end_date)]\n",
    "\n",
    "# Ensure the returns and factors align on the same dates\n",
    "common_dates = returns.index.intersection(factors.index)\n",
    "returns = returns.loc[common_dates]\n",
    "factors = factors.loc[common_dates]\n",
    "\n",
    "try:\n",
    "    factor_exposures = calculate_factor_exposures(returns, factors)\n",
    "    print(\"Factor exposures:\")\n",
    "    print(factor_exposures.head())\n",
    "\n",
    "    # Convert factor exposures to float type\n",
    "    factor_exposures = factor_exposures.astype(float)\n",
    "\n",
    "    # Clip extreme values for better visualization\n",
    "    lower_bound, upper_bound = np.percentile(factor_exposures.values, [1, 99])\n",
    "    factor_exposures_clipped = factor_exposures.clip(lower_bound, upper_bound)\n",
    "\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    sns.heatmap(factor_exposures_clipped, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "    plt.title('Factor Exposures Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\nFactor Exposures Summary Statistics:\")\n",
    "    print(factor_exposures.describe())\n",
    "\n",
    "    # Print extreme values\n",
    "    print(\"\\nStocks with Extreme Factor Exposures:\")\n",
    "    for column in factor_exposures.columns:\n",
    "        max_stock = factor_exposures[column].idxmax()\n",
    "        min_stock = factor_exposures[column].idxmin()\n",
    "        print(f\"\\n{column}:\")\n",
    "        print(f\"  Max: {max_stock} ({factor_exposures.loc[max_stock, column]:.2f})\")\n",
    "        print(f\"  Min: {min_stock} ({factor_exposures.loc[min_stock, column]:.2f})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Unable to process or visualize Fama-French data. Please check the data and format.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33931ce8",
   "metadata": {},
   "source": [
    "## 3. Portfolio Construction Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814546c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def equal_weight_portfolio(returns):\n",
    "    n = returns.shape[1]\n",
    "    weights = np.ones(n) / n\n",
    "    return weights\n",
    "\n",
    "def risk_parity_portfolio(returns, risk_target=0.1, max_iter=1000, tolerance=1e-6):\n",
    "    n = returns.shape[1]\n",
    "    weights = np.ones(n) / n\n",
    "    cov_matrix = returns.cov().values\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        risk_contributions = weights * (cov_matrix.dot(weights))\n",
    "        new_weights = weights * risk_target / risk_contributions\n",
    "        new_weights /= np.sum(new_weights)\n",
    "        \n",
    "        if np.sum(np.abs(new_weights - weights)) < tolerance:\n",
    "            break\n",
    "        \n",
    "        weights = new_weights\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def min_variance_portfolio(returns):\n",
    "    cov_matrix = returns.cov().values\n",
    "    n = returns.shape[1]\n",
    "    \n",
    "    def objective(weights):\n",
    "        return weights.T.dot(cov_matrix).dot(weights)\n",
    "    \n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(n))\n",
    "    \n",
    "    result = minimize(objective, np.ones(n) / n, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "def mean_variance_portfolio(returns, risk_aversion=2):\n",
    "    mean_returns = returns.mean().values\n",
    "    cov_matrix = returns.cov().values\n",
    "    n = returns.shape[1]\n",
    "    \n",
    "    def objective(weights):\n",
    "        portfolio_return = np.dot(weights, mean_returns)\n",
    "        portfolio_variance = np.dot(weights.T, np.dot(cov_matrix, weights))\n",
    "        return -portfolio_return + risk_aversion * portfolio_variance\n",
    "    \n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(n))\n",
    "    \n",
    "    result = minimize(objective, np.ones(n) / n, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "# Calculate portfolio weights\n",
    "equal_weight = equal_weight_portfolio(returns)\n",
    "risk_parity = risk_parity_portfolio(returns)\n",
    "min_var = min_variance_portfolio(returns)\n",
    "mean_var = mean_variance_portfolio(returns)\n",
    "\n",
    "portfolios = {\n",
    "    'Equal Weight': equal_weight,\n",
    "    'Risk Parity': risk_parity,\n",
    "    'Minimum Variance': min_var,\n",
    "    'Mean-Variance': mean_var\n",
    "}\n",
    "\n",
    "# Visualize portfolio weights\n",
    "plt.figure(figsize=(15, 10))\n",
    "for name, weights in portfolios.items():\n",
    "    plt.bar(range(len(weights)), weights, alpha=0.5, label=name)\n",
    "plt.xlabel('Stocks')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Portfolio Weights Comparison')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(dax40_tickers)), dax40_tickers, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c48d20",
   "metadata": {},
   "source": [
    "## 4. Litterman's Approach for Expected Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092fb9e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def black_litterman_model(returns, market_caps, risk_aversion=2.5, tau=0.05):\n",
    "    # Calculate historical returns and covariance\n",
    "    hist_returns = returns.mean()\n",
    "    cov_matrix = returns.cov()\n",
    "    \n",
    "    # Calculate market-implied equilibrium returns\n",
    "    market_weights = market_caps / market_caps.sum()\n",
    "    implied_returns = risk_aversion * cov_matrix.dot(market_weights)\n",
    "    \n",
    "    # Combine views with prior\n",
    "    posterior_cov = np.linalg.inv(np.linalg.inv(tau * cov_matrix) + np.linalg.inv(cov_matrix))\n",
    "    posterior_returns = posterior_cov.dot(np.linalg.inv(tau * cov_matrix).dot(implied_returns) + np.linalg.inv(cov_matrix).dot(hist_returns))\n",
    "    \n",
    "    return pd.Series(posterior_returns, index=returns.columns)\n",
    "\n",
    "# Get market cap data (this is simplified, you might want to use actual market cap data)\n",
    "market_caps = dax40_data['Adj Close'].iloc[-1] * dax40_data['Volume'].iloc[-1]\n",
    "\n",
    "expected_returns = black_litterman_model(returns, market_caps)\n",
    "\n",
    "print(\"Expected returns using Litterman's approach:\")\n",
    "print(expected_returns)\n",
    "\n",
    "# Visualize expected returns\n",
    "plt.figure(figsize=(12, 6))\n",
    "expected_returns.sort_values().plot(kind='bar')\n",
    "plt.title(\"Expected Returns (Litterman's Approach)\")\n",
    "plt.xlabel('Stocks')\n",
    "plt.ylabel('Expected Return')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47f6c1",
   "metadata": {},
   "source": [
    "## 5. Monte Carlo Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08176e0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def monte_carlo_simulation(returns, weights, num_simulations=10000, time_horizon=252):\n",
    "    mean_returns = returns.mean()\n",
    "    cov_matrix = returns.cov()\n",
    "    \n",
    "    simulated_returns = np.random.multivariate_normal(mean_returns, cov_matrix, (num_simulations, time_horizon))\n",
    "    portfolio_returns = np.dot(simulated_returns, weights)\n",
    "    cumulative_returns = np.cumprod(1 + portfolio_returns, axis=1)\n",
    "    \n",
    "    return cumulative_returns\n",
    "\n",
    "# Run simulations for each portfolio\n",
    "simulations = {name: monte_carlo_simulation(returns, weights) for name, weights in portfolios.items()}\n",
    "\n",
    "# Visualize simulations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "fig.suptitle('Monte Carlo Simulations of Portfolio Performance', fontsize=16)\n",
    "\n",
    "for (name, sim), ax in zip(simulations.items(), axes.ravel()):\n",
    "    for i in range(100):  # Plot 100 random simulations\n",
    "        ax.plot(sim[i], alpha=0.1, color='blue')\n",
    "    ax.plot(sim.mean(axis=0), color='red', linewidth=2)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Trading Days')\n",
    "    ax.set_ylabel('Cumulative Returns')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4e81a",
   "metadata": {},
   "source": [
    "## 6. Performance Evaluation and Comparison with DAX Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1663c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_portfolio_returns(weights, returns):\n",
    "    return returns.dot(weights)\n",
    "\n",
    "def calculate_sharpe_ratio(returns, risk_free_rate=0):\n",
    "    excess_returns = returns - risk_free_rate\n",
    "    return np.sqrt(252) * excess_returns.mean() / excess_returns.std()\n",
    "\n",
    "# Load DAX index data\n",
    "dax_index = yf.download('^GDAXI', start=start_date, end=end_date)['Adj Close']\n",
    "dax_returns = dax_index.pct_change().dropna()\n",
    "\n",
    "# Calculate cumulative returns for each portfolio and the DAX index\n",
    "cumulative_returns = pd.DataFrame({name: (1 + calculate_portfolio_returns(weights, returns)).cumprod()\n",
    "                                   for name, weights in portfolios.items()})\n",
    "cumulative_returns['DAX Index'] = (1 + dax_returns).cumprod()\n",
    "\n",
    "# Visualize performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "cumulative_returns.plot()\n",
    "plt.title('Portfolio Performance Comparison with DAX Index')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate Sharpe ratios\n",
    "sharpe_ratios = {}\n",
    "for name, weights in portfolios.items():\n",
    "    portfolio_returns = calculate_portfolio_returns(weights, returns)\n",
    "    sharpe_ratios[name] = calculate_sharpe_ratio(portfolio_returns)\n",
    "sharpe_ratios['DAX Index'] = calculate_sharpe_ratio(dax_returns)\n",
    "\n",
    "print(\"Sharpe Ratios:\")\n",
    "for name, ratio in sharpe_ratios.items():\n",
    "    print(f\"{name}: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8f259",
   "metadata": {},
   "source": [
    "## 7. Time-Specific Sharpe Ratio Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d16a9c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_rolling_sharpe_ratio(returns, window=63, risk_free_rate=0.015):  # Using ~3 months window\n",
    "    excess_returns = returns - risk_free_rate\n",
    "    rolling_return = excess_returns.rolling(window=window).mean()\n",
    "    rolling_std = excess_returns.rolling(window=window).std()\n",
    "    return np.sqrt(252) * rolling_return / rolling_std\n",
    "\n",
    "# Calculate portfolio returns\n",
    "portfolio_returns = pd.DataFrame({name: calculate_portfolio_returns(weights, returns)\n",
    "                                  for name, weights in portfolios.items()})\n",
    "\n",
    "# Add DAX returns\n",
    "portfolio_returns['DAX Index'] = dax_returns\n",
    "\n",
    "# Calculate rolling Sharpe ratios\n",
    "rolling_sharpe_ratios = pd.DataFrame({name: calculate_rolling_sharpe_ratio(returns)\n",
    "                                      for name, returns in portfolio_returns.items()})\n",
    "\n",
    "# Visualize rolling Sharpe ratios\n",
    "plt.figure(figsize=(16, 8))\n",
    "for column in rolling_sharpe_ratios.columns:\n",
    "    plt.plot(rolling_sharpe_ratios.index, rolling_sharpe_ratios[column], label=column)\n",
    "\n",
    "plt.title('Rolling Sharpe Ratios (3-Month Window)', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Sharpe Ratio', fontsize=12)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nRolling Sharpe Ratios Summary Statistics:\")\n",
    "print(rolling_sharpe_ratios.describe())\n",
    "\n",
    "# Print average Sharpe ratios\n",
    "print(\"\\nAverage Sharpe Ratios:\")\n",
    "print(rolling_sharpe_ratios.mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c66ba",
   "metadata": {},
   "source": [
    "## 8. Sector Allocation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d62347",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_sector_weights(portfolio_weights, ticker_to_sector):\n",
    "    sector_weights = {}\n",
    "    for ticker, weight in zip(returns.columns, portfolio_weights):\n",
    "        sector = ticker_to_sector[ticker]\n",
    "        sector_weights[sector] = sector_weights.get(sector, 0) + weight\n",
    "    return sector_weights\n",
    "\n",
    "# Calculate sector weights for each portfolio\n",
    "sector_allocations = {name: calculate_sector_weights(weights, ticker_to_sector) \n",
    "                      for name, weights in portfolios.items()}\n",
    "\n",
    "# Visualize sector allocations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "fig.suptitle('Sector Allocations by Portfolio Strategy', fontsize=16)\n",
    "\n",
    "for (name, allocation), ax in zip(sector_allocations.items(), axes.ravel()):\n",
    "    sectors = list(allocation.keys())\n",
    "    weights = list(allocation.values())\n",
    "    ax.pie(weights, labels=sectors, autopct='%1.1f%%', startangle=90)\n",
    "    ax.set_title(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d760d",
   "metadata": {},
   "source": [
    "## 9. Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b516458a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming we have the summary dataframe from the previous code\n",
    "summary = pd.DataFrame({\n",
    "    'Volatility': volatilities,\n",
    "    'VaR (95%)': var_values,\n",
    "    'CVaR (95%)': cvar_values,\n",
    "    'Sharpe Ratio': sharpe_ratios\n",
    "})\n",
    "\n",
    "# Color palette\n",
    "colors = sns.color_palette(\"husl\", n_colors=len(summary))\n",
    "\n",
    "# 1. Bar plots for each metric\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "fig.suptitle('Portfolio Risk Metrics Comparison', fontsize=16)\n",
    "\n",
    "metrics = ['Volatility', 'VaR (95%)', 'CVaR (95%)', 'Sharpe Ratio']\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    summary[metric].plot(kind='bar', ax=ax, color=colors)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    for j, v in enumerate(summary[metric]):\n",
    "        ax.text(j, v, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Risk-Return Scatter Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, (index, row) in enumerate(summary.iterrows()):\n",
    "    plt.scatter(row['Volatility'], row['Sharpe Ratio'], s=100, color=colors[i], label=index)\n",
    "    plt.annotate(index, (row['Volatility'], row['Sharpe Ratio']), xytext=(5, 5), \n",
    "                 textcoords='offset points', fontsize=8, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Annualized Volatility')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.title('Risk-Return Trade-off')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. VaR vs CVaR Comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "x = range(len(summary))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x, summary['VaR (95%)'], width, label='VaR (95%)', color='skyblue', alpha=0.7)\n",
    "plt.bar([i + width for i in x], summary['CVaR (95%)'], width, label='CVaR (95%)', color='salmon', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Portfolio Strategy')\n",
    "plt.ylabel('Value')\n",
    "plt.title('VaR vs CVaR Comparison')\n",
    "plt.xticks([i + width/2 for i in x], summary.index, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "for i, (var, cvar) in enumerate(zip(summary['VaR (95%)'], summary['CVaR (95%)'])):\n",
    "    plt.text(i, var, f'{var:.4f}', ha='center', va='bottom')\n",
    "    plt.text(i + width, cvar, f'{cvar:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee31c5",
   "metadata": {},
   "source": [
    "## 10. Reallocation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def calculate_turnover(old_weights, new_weights):\n",
    "    return np.sum(np.abs(new_weights - old_weights)) / 2\n",
    "\n",
    "def equal_weight_portfolio(returns):\n",
    "    return np.ones(returns.shape[1]) / returns.shape[1]\n",
    "\n",
    "def risk_parity_portfolio(returns, risk_target=1, max_iter=1000, tolerance=1e-6):\n",
    "    n = returns.shape[1]\n",
    "    weights = np.ones(n) / n\n",
    "    cov_matrix = returns.cov().values\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        risk_contributions = weights * (cov_matrix.dot(weights))\n",
    "        new_weights = weights * risk_target / risk_contributions\n",
    "        new_weights /= np.sum(new_weights)\n",
    "        \n",
    "        if np.sum(np.abs(new_weights - weights)) < tolerance:\n",
    "            break\n",
    "        \n",
    "        weights = new_weights\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def min_variance_portfolio(returns):\n",
    "    n = returns.shape[1]\n",
    "    cov_matrix = returns.cov().values\n",
    "    \n",
    "    def objective(weights):\n",
    "        return weights.T @ cov_matrix @ weights\n",
    "    \n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(n))\n",
    "    \n",
    "    result = minimize(objective, np.ones(n) / n, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "def mean_variance_portfolio(returns, risk_aversion=1):\n",
    "    n = returns.shape[1]\n",
    "    mean_returns = returns.mean().values\n",
    "    cov_matrix = returns.cov().values\n",
    "    \n",
    "    def objective(weights):\n",
    "        portfolio_return = weights.T @ mean_returns\n",
    "        portfolio_variance = weights.T @ cov_matrix @ weights\n",
    "        return -portfolio_return + risk_aversion * portfolio_variance\n",
    "    \n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(n))\n",
    "    \n",
    "    result = minimize(objective, np.ones(n) / n, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "portfolio_strategies = {\n",
    "    'Equal Weight': equal_weight_portfolio,\n",
    "    'Risk Parity': risk_parity_portfolio,\n",
    "    'Minimum Variance': min_variance_portfolio,\n",
    "    'Mean-Variance': mean_variance_portfolio\n",
    "}\n",
    "\n",
    "reallocation_dates = pd.date_range(start=returns.index[0], end=returns.index[-1], freq='QE')\n",
    "turnover = {name: [] for name in portfolio_strategies.keys()}\n",
    "\n",
    "for i in range(1, len(reallocation_dates)):\n",
    "    start_date = reallocation_dates[i-1]\n",
    "    end_date = reallocation_dates[i]\n",
    "    old_returns = returns.loc[:start_date]\n",
    "    new_returns = returns.loc[:end_date]\n",
    "    \n",
    "    for name, strategy in portfolio_strategies.items():\n",
    "        old_weights = strategy(old_returns)\n",
    "        new_weights = strategy(new_returns)\n",
    "        turnover[name].append(calculate_turnover(old_weights, new_weights))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, turnovers in turnover.items():\n",
    "    plt.plot(reallocation_dates[1:], turnovers, label=name, marker='o')\n",
    "\n",
    "plt.title('Portfolio Turnover over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Turnover')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "avg_turnover = {name: np.mean(turnovers) for name, turnovers in turnover.items()}\n",
    "\n",
    "print(\"Average Turnover:\")\n",
    "for name, avg in avg_turnover.items():\n",
    "    print(f\"{name}: {avg:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(avg_turnover.keys(), avg_turnover.values())\n",
    "plt.title('Average Portfolio Turnover')\n",
    "plt.xlabel('Portfolio Strategy')\n",
    "plt.ylabel('Average Turnover')\n",
    "plt.xticks(rotation=45)\n",
    "for i, v in enumerate(avg_turnover.values()):\n",
    "    plt.text(i, v, f'{v:.4f}', ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d33ef6",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb98005",
   "metadata": {},
   "source": [
    "This comprehensive analysis of the DAX 40 companies includes:\n",
    "1. Sector-based performance analysis\n",
    "2. Fama-French five-factor model implementation\n",
    "3. Various portfolio construction techniques\n",
    "4. Litterman's approach for expected returns\n",
    "5. Monte Carlo simulations for robustness checks\n",
    "6. Performance comparison with the DAX index\n",
    "7. Time-specific Sharpe ratio calculations\n",
    "8. Sector allocation analysis\n",
    "9. Risk analysis including volatility and Value at Risk (VaR)\n",
    "10. Reallocation analysis to assess portfolio turnover\n",
    "\n",
    "These results provide deep insights into the effectiveness of different portfolio construction techniques in the German stock market, considering both factor-based and sector-based approaches. The analysis also compares the performance of these strategies to the DAX index, evaluates their risk-adjusted returns over time, and assesses their practical implications in terms of sector exposure and reallocation costs.\n",
    "\n",
    "Key findings and observations:\n",
    "- [Insert your main findings and observations here based on the results]\n",
    "\n",
    "Areas for further research:\n",
    "- Incorporate transaction costs and taxes into the portfolio optimization process\n",
    "- Explore machine learning approaches for return prediction and portfolio optimization\n",
    "- Investigate the impact of different economic regimes on portfolio performance\n",
    "- Conduct out-of-sample testing to assess the robustness of the strategies"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
